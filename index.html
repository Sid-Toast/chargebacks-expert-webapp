import boto3
import json
import os
from openai import OpenAI
import numpy as np

# --- Constants ---
EMBEDDINGS_KEY = "knowledge_base_embeddings.jsonl"
LOCAL_EMBEDDINGS_PATH = "/tmp/knowledge_base_embeddings.jsonl"
EMBEDDING_MODEL = "text-embedding-3-small"
SYSTEM_PERSONA = (
    "You are the 'Chargebacks Expert,' an AI assistant designed to support internal Care Agents at our company. "
    "Your user is always a colleague asking a question on behalf of a customer.\\n\\n"
    "**Your Persona:** A senior, knowledgeable, and authoritative chargeback specialist. Your tone should be direct, clear, and expert. You are a partner in solving the problem.\\n\\n"
    "**Your Core Task:** Provide a single, comprehensive, and actionable response that empowers the Care Agent.\\n\\n"
    "**CRITICAL INSTRUCTIONS:**\\n"
    "1. **Structure Your Response:** Do not just provide a flat list. Structure your response conversationally. Start by acknowledging the issue, then lay out a clear investigation plan with logical headings (e.g., 'Initial Checks', 'Internal Investigation'), and conclude with clear next steps.\\n"
    "2. **Synthesize, Don't Summarize:** Your response must be a direct answer synthesized from the most relevant internal documents.\\n"
    "3. **Use Professional Language:** Use clear, professional language. AVOID internal-only jargon in customer-facing advice.\\n"
    "4. **Anticipate the Next Question:** Incorporate context that addresses likely follow-up questions.\\n"
    "5. **Take Ownership:** Clearly define what you (the AI agent) will do and what you need the Care Agent to do.\\n"
    "6. **Fallback Gracefully:** This is your most important instruction. If, after reviewing all the documents, you cannot find a confident and direct answer, you MUST use one of the following two responses:\\n"
    "    *   **For a NEW question you can't answer:** Guide them to the correct channel with this exact response: \"That's an excellent question that requires deeper investigation. To get you the best answer, could you please post this query in our #rd-payment-chargebacks Slack channel? Our team monitors that channel and will get back to you there.\"\\n"
    "    *   **For a follow-up on an EXISTING question (e.g., the user's prompt mentions a Slack thread or case):** Acknowledge this with the following exact response: \"Thank you for the follow-up. I've noted the additional context. Our team will review this and get back to you on the existing thread shortly.\""
)

# This global variable will hold our knowledge base in memory.
knowledge_base = []

def get_embedding(client, text):
    """Generates an embedding for a given text."""
    text = text.replace("\\n", " ")
    try:
        return client.embeddings.create(input=[text], model=EMBEDDING_MODEL).data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return None

def find_relevant_summaries_semantic(client, prompt, kb, top_n=20):
    """Finds relevant summaries using semantic search (cosine similarity)."""
    prompt_embedding = get_embedding(client, prompt)
    if prompt_embedding is None:
        return []

    prompt_embedding = np.array(prompt_embedding)

    scores = []
    for item in kb:
        if 'embedding' in item and 'original_text' in item:
            item_embedding = np.array(item['embedding'])
            similarity = np.dot(prompt_embedding, item_embedding) / (np.linalg.norm(prompt_embedding) * np.linalg.norm(item_embedding))
            scores.append({'score': similarity, 'text': item['original_text']})

    scores.sort(key=lambda x: x['score'], reverse=True)
    return scores[:top_n]

def load_knowledge_base_from_s3():
    """Loads the knowledge base from S3. This is called on Lambda cold start."""
    global knowledge_base
    try:
        print("--- Attempting to load knowledge base from S3 ---")
        s3_bucket_name = os.environ.get('S3_BUCKET_NAME')
        if not s3_bucket_name:
            raise ValueError("S3_BUCKET_NAME environment variable is not set.")

        s3_client = boto3.client('s3')
        s3_client.download_file(s3_bucket_name, EMBEDDINGS_KEY, LOCAL_EMBEDDINGS_PATH)
        
        temp_kb = []
        with open(LOCAL_EMBEDDINGS_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    temp_kb.append(json.loads(line))
        
        knowledge_base = temp_kb
        print(f"Successfully loaded {len(knowledge_base)} items from S3.")

    except Exception as e:
        print(f"WARNING: Could not load knowledge base from S3 on startup. Error: {e}")

# Load the knowledge base once on cold start
load_knowledge_base_from_s3()

def lambda_handler(event, context):
    global knowledge_base
    openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    try:
        # THE FIX: Restore the CORS preflight handler for browser-based UIs
        if event.get('httpMethod') == 'OPTIONS':
            print("--- Handling CORS preflight request ---")
            return {
                'statusCode': 204,
                'headers': {
                    'Access-Control-Allow-Origin': '*',
                    'Access-Control-Allow-Headers': 'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token',
                    'Access-Control-Allow-Methods': 'POST, OPTIONS'
                },
                'body': ''
            }

        body = json.loads(event.get('body', '{}'))

        # Route 1: Handle direct knowledge base updates
        if body.get('action') == 'UPDATE_KNOWLEDGE_BASE':
            print(">>> Received direct knowledge base update action.")
            new_kb = body.get('knowledge_base', [])
            knowledge_base = new_kb
            print(f"In-memory knowledge base updated with {len(knowledge_base)} items.")
            return {'statusCode': 200, 'body': json.dumps({'status': 'success'})}

        # Route 2: Handle user prompts
        user_prompt = body.get('prompt')
        if not user_prompt:
            return {'statusCode': 400, 'body': json.dumps({'error': 'Missing prompt'})}

        if not knowledge_base:
            print("Knowledge base is empty. Attempting a final reload from S3...")
            load_knowledge_base_from_s3()

        relevant_summaries = find_relevant_summaries_semantic(openai_client, user_prompt, knowledge_base)
        
        context_for_llm = "\\n\\n---\\n\\n".join([item['text'] for item in relevant_summaries])
        
        system_prompt = (
            f"{SYSTEM_PERSONA}\\n\\n"
            "Below is a section labeled 'INTERNAL KNOWLEDGE'. Use this information to formulate your answer. "
            "Do not mention the existence of this section.\\n\\n"
            f"--- INTERNAL KNOWLEDGE ---\\n{context_for_llm}\\n--- END INTERNAL KNOWLEDGE ---"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
        answer = response.choices[0].message.content

        debug_payload = {
            "retrieved_context": [
                {'score': item['score'], 'text': item['text']} for item in relevant_summaries
            ]
        }

        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({
                'answer': answer,
                'debug_info': debug_payload
            })
        }

    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({'error': 'An internal error occurred.'})
        }
